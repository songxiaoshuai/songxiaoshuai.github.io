<html>

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<meta name="description" content="Homepage of SXS">
	<link rel="stylesheet" href="./files/jemdoc.css" type="text/css">
	<title>Xiaoshuai Song's Homepage|宋晓帅的个人主页</title>
</head>


<body>

<div id="layout-content" style="margin-top:25px">

<table><tbody><tr>
    <td width="670">
        <div id="toptitle"><h1>Xiaoshuai Song (宋晓帅)&nbsp;</h1></div>
        <h3>Master Student</h3>  
        <p>
            Room 731, 3rd Teaching Building <br>
            Dept. of Artificial Intelligence <br>
            Beijing University of Posts and Telecommunications (BUPT) <br>
            Beijing, China, 100084. <br>
            Email:  <a href="mailto:songxiaoshuai@bupt.edu.cn">songxiaoshuai@bupt.edu.cn</a> <br>

<a href="https://scholar.google.com/citations?user=0QqMGUQAAAAJ">[Google Scholar]</a>
<a href="https://www.semanticscholar.org/author/Xiaoshuai-Song/2218475861">[Semantic Scholar]</a>
        </p>
    </td>
    
    <td><img src="./files/photo.jpg" border="0" width="120"></td>
</tr></tbody></table>


<h2>Biography</h2>
    <p>I am a final-year master student of <a href="https://pris-nlp.github.io/en/#hero">Pattern Recognition and Intelligent System Laboratory</a> in the Department of Artificial Intelligence</a>, <a href="https://www.bupt.edu.cn/">Beijing University of Posts and Telecommunications(BUPT)</a>, advised by <a href="https://pris-nlp.github.io/en/author/weiran-xu/">Prof. Weiran Xu</a>. Previously, I received my B.S. degree from the Department of Information and Communication Engineering, Beijing University of Posts and Telecommunications in July, 2022.</p>
    
    <p> I used to focus on Out-of-domain Intent Mining (Detection, Discovery,Increment) in Dialog System. Currently, my research interests focuses on external knowledge-enhanced LLMs, including retrieval-augmented knowledge editing and retrieval-augmented generation, as well as exploring the capabilities of LLMs.</p>

<h2>News</h2>
<ul>
    <li><strong>[2025-01]</strong> One paper has been accepted by WWW 2025, and two papers has been accepted by ICLR 2025! </li>
    <li><strong>[2024-06]</strong> We propose CS-Bench,  the first benchmark dedicated to evaluating the performance of LLMs in computer science! See more <a href='https://csbench.github.io/'>details</a>. </li>
</ul>

	
<h2>Experiences</h2>
    <strong>Academia</strong>
    <ul>
    <li>[2022.9 - Now] M.S. at Beijing University of Posts and Telecommunications</li>
    <li>[2018.9 - 2022.6] B.S. at Beijing University of Posts and Telecommunications</li>
</ul>
    <strong>Industry</strong>
    <ul>
    <li>[2024.7 - Now] Alibaba, Taobao & TMall - Future Life Lab, LLM Algorithm Research Intern. </li>
    <li>[2023.7 - 2024.6] SenseTime, Research Institute - NLP, Algorithm Research Intern. </li>
</ul>



<h2>Publications</h2>

(* denotes equal contributions)<br>

<strong>-2025-</strong>

<ul><li><p>CS-Bench: A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery<br />
    <span style="font-size: 0.8em; color: #4D4D4D;"><strong>Xiaoshuai Song</strong>, Muxi Diao, Guanting Dong, Zhengyang Wang, Yujia Fu, Runqi Qiao, Zhexu Wang, Dayuan Fu, Huangxuan Wu, Bin Liang, Weihao Zeng, Yejie Wang, Zhuoma GongQue, Jianing Yu, Qiuna Tan, Weiran Xu</span><br />
    ICLR 2025. 
    <a href='https://arxiv.org/pdf/2406.08587.pdf'>[paper]</a>
	<a href='https://csbench.github.io/'>[website]</a>
</li>
</ul>

<ul><li><p>MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models<br />
    <span style="font-size: 0.8em; color: #4D4D4D;">Pei Wang, Yanan Wu, Zekun Wang, Jiaheng Liu, <strong>Xiaoshuai Song</strong>, Zhongyuan Peng, Ken Deng, Chenchen Zhang, Jiakai Wang, Junran Peng, Ge Zhang, Hangyu Guo, Zhaoxiang Zhang, Wenbo Su, Bo Zheng</span><br />
    ICLR 2025. 
    <a href='https://arxiv.org/pdf/2410.11710.pdf'>[paper]</a>
</li>
</ul>

<ul><li><p>Assessing and Post-Processing Black Box Large Language Models for Knowledge Editing<br />
    <span style="font-size: 0.8em; color: #4D4D4D;"><strong>Xiaoshuai Song</strong>, Zhengyang Wang, Keqing He, Guanting Dong, Jinxu Zhao, Weiran Xu</span><br />
    WWW 2025 Oral. 
    <a href='https://arxiv.org/pdf/2402.08631.pdf'>[paper]</a>
</li>
</ul>
  
<strong>-2024-</strong>

<ul><li><p>Toward General Instruction-Following Alignment for Retrieval-Augmented Generation<br />
    <span style="font-size: 0.8em; color: #4D4D4D;">Guanting Dong, <strong>Xiaoshuai Song</strong>, Yutao Zhu, Runqi Qiao, Zhicheng Dou, Ji-Rong Wen</span></span><br />
    AAAI 2025. 
    <a href='https://arxiv.org/pdf/2410.09584'>[paper]</a>
	<a href='https://followrag.github.io/'>[website]</a>
</li>
</ul>

<ul><li><p>Faceptor: A Generalist Model for Face Perception<br />
<span style="font-size: 0.8em; color: #4D4D4D;">Lixiong Qin, Mei Wang, Xuannan Liu, Yuhang Zhang, Wei Deng, <strong>Xiaoshuai Song</strong>, Weiran Xu, and Weihong Deng</span><br />
ECCV 2024 Oral. 
<a href='https://arxiv.org/pdf/2403.09500'>[paper]</a>
</li>
</ul>

<ul><li><p>Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent Detection<br />
<span style="font-size: 0.8em; color: #4D4D4D;">Pei Wang, Keqing He, Yejie Wang, <strong>Xiaoshuai Song</strong>, Yutao Mou, Jingang Wang, Yunsen Xian, Xunliang Cai, Weiran Xu</span><br />
LERC-COLING 2024.
<a href='https://arxiv.org/pdf/2402.17256.pdf'>[paper]</a>
</li>
</ul>
	
<ul><li><p>Noise-BERT: A Unified Perturbation-robust Framework With Noise Alignment Pre-training For Noisy Slot Filing Task<br />
<span style="font-size: 0.8em; color: #4D4D4D;">Jinxu Zhao, Guanting Dong, Yueyan Qiu, Tingfeng Hui, <strong>Xiaoshuai Song</strong>, Daichi Guo, Weiran Xu</span><br />
ICASSP 2024. 
<a href='https://arxiv.org/pdf/2402.14494.pdf'>[paper]</a>
</li>
</ul>
 
<strong>-2023-</strong>

<ul><li><p>Large Language Models Meet Open-World Intent Discovery and Recognition: An Evaluation of ChatGPT<br />
<span style="font-size: 0.8em; color: #4D4D4D;"><strong>Xiaoshuai Song</strong>, Keqing He, Pei Wang, Guanting Dong, Yutao Mou, Jingang Wang, Yunsen Xian, Xunliang Cai, Weiran Xu</span><br />
Main Conference of EMNLP 2023. 
<a href='https://aclanthology.org/2023.emnlp-main.636.pdf'>[paper]</a>
</li>
</ul>


<ul><li><p>Continual Generalized Intent Discovery: Marching Towards Dynamic and Open-world Intent Recognition<br />
<span style="font-size: 0.8em; color: #4D4D4D;"><strong>Xiaoshuai Song</strong>, Yutao Mou, Keqing He, Yueyan Qiu, Pei Wang, Weiran Xu</span><br />
Findings of EMNLP 2023. 
<a href='https://aclanthology.org/2023.findings-emnlp.289.pdf'>[paper]</a>
</li>
</ul>

<ul><li><p>APP: Adaptive Prototypical Pseudo-Labeling for Few-shot OOD Detection<br />
<span style="font-size: 0.8em; color: #4D4D4D;">Pei Wang, Keqing He, Yutao Mou, <strong>Xiaoshuai Song</strong>, Yanan Wu, Jingang Wang, Yunsen Xian, Xunliang Cai, Weiran Xu</span><br />
Findings of EMNLP 2023. 
<a href='https://aclanthology.org/2023.findings-emnlp.258.pdf'>[paper]</a>
</li>
</ul>

<ul><li><p>Decoupling Pseudo Label Disambiguation and Representation Learning for Generalized Intent Discovery<br />
<span style="font-size: 0.8em; color: #4D4D4D;">Yutao Mou, <strong>Xiaoshuai Song*</strong>, Keqing He, Chen Zeng, Pei Wang, Jingang Wang, Yunsen Xian, Weiran Xu</span><br />
Main Conference of ACL 2023. 
<a href='https://aclanthology.org/2023.acl-long.538.pdf'>[paper]</a>
</li>
</ul>



<h2>Preprints</h2>

<ul><li><p>ProgCo: Program Helps Self-Correction of Large Language Models<br />
    <span style="font-size: 0.8em; color: #4D4D4D;"><strong>Xiaoshuai Song</strong>, Yanan Wu, Weixun Wang, Jiaheng Liu, Wenbo Su, Bo Zheng</span><br />
    Arxiv 25.01. 
    <a href='https://arxiv.org/pdf/2501.01264.pdf'>[paper]</a>
</li>
</ul>

<ul><li><p>We-Math: Does Your Large Multimodal Model Achieve Human-like Mathematical Reasoning?<br />
    <span style="font-size: 0.8em; color: #4D4D4D;">Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, <strong>Xiaoshuai Song</strong>, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, Runfeng Qiao, Yifan Zhang, Xiao Zong, Yida Xu, Muxi Diao, Zhimin Bao, Chen Li, Honggang Zhang</span><br />
    Arxiv 24.07. 
    <a href='https://arxiv.org/pdf/2407.01284'>[paper]</a>
	<a href='https://we-math.github.io/'>[website]</a>
</li>
</ul>

<ul><li><p>Towards Robust and Generalizable Training: An Empirical Study of Noisy Slot Filling for Input Perturbations<br />
    <span style="font-size: 0.8em; color: #4D4D4D;">Jiachi Liu, Liwen Wang, Guanting Dong, <strong>Xiaoshuai Song</strong>, Zechen Wang, Zhengyang Wang, Shanglin Lei, Jinzheng Zhao, Keqing He, Bo Xiao, Weiran Xu</span><br />
    Arxiv 23.10. 
    <a href='https://arxiv.org/pdf/2310.03518.pdf'>[paper]</a>
</li>
</ul>

<h2>Honors & Awards</h2>
<ul>
    <li>
        <strong>National Scholarship (Top 1%), Ministry of Education.</strong> 2024
    </li>
    <li>
        <strong>Outstanding Master Students (Top 5%), BUPT.</strong> 2023, 2024
    </li>
    <li>
        <strong>Excellent First-class Scholarship for Master Students, BUPT.</strong> 2022, 2023
    </li>
    <li>
        <strong>Chinese College Student Mathematics Competition (Preliminary round), First Prize</strong>, 2022.01
    </li>
    <li>
        <strong>MathorCup University Mathematical Modeling Challenge, Second Prize</strong>, 2021.03
    </li>
    <li>
        <strong>The Mathematical Contest in Modeling, Honorable Mention</strong>, 2020.4
    </li>

</ul>


<div id="footer">
	<div id="footer-text"></div>
</div>
&copy 2025 Xiaoshuai Song

</body>

</html>
